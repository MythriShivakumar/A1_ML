{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np \n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "import pickle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Penguin Data \n",
      "        calorie requirement  average sleep duration  bill_length_mm  \\\n",
      "count           344.000000              344.000000      337.000000   \n",
      "mean           5270.002907               10.447674       45.494214   \n",
      "std            1067.959116                2.265895       10.815787   \n",
      "min            3504.000000                7.000000       32.100000   \n",
      "25%            4403.000000                9.000000       39.500000   \n",
      "50%            5106.500000               10.000000       45.100000   \n",
      "75%            6212.750000               12.000000       49.000000   \n",
      "max            7197.000000               14.000000      124.300000   \n",
      "\n",
      "       bill_depth_mm  flipper_length_mm  body_mass_g         year  \n",
      "count     333.000000         336.000000   339.000000   342.000000  \n",
      "mean       18.018318         197.764881  4175.463127  2008.035088  \n",
      "std         9.241384          27.764491   858.713267     0.816938  \n",
      "min        13.100000          10.000000   882.000000  2007.000000  \n",
      "25%        15.700000         190.000000  3550.000000  2007.000000  \n",
      "50%        17.300000         197.000000  4050.000000  2008.000000  \n",
      "75%        18.700000         213.000000  4750.000000  2009.000000  \n",
      "max       127.260000         231.000000  6300.000000  2009.000000  \n",
      "\n",
      "Diamond Data \n",
      "        average us salary  number of diamonds mined (millions)\n",
      "count       53940.000000                         53940.000000\n",
      "mean        39521.990100                             2.902669\n",
      "std          5486.892971                             1.325985\n",
      "min         30000.000000                             0.600000\n",
      "25%         34780.000000                             1.750000\n",
      "50%         39547.500000                             2.910000\n",
      "75%         44252.000000                             4.050000\n",
      "max         48999.000000                             5.200000\n",
      "\n",
      "Emissions Dataset\n",
      "                Year         Total   Temperature  GDP Per Capita (USD)  \\\n",
      "count  62307.000000  62381.000000  63104.000000          63104.000000   \n",
      "mean    1888.267097     73.683456     49.497813          39026.539015   \n",
      "std      122.651184    843.930381     17.292092          10975.539432   \n",
      "min     1003.000000      0.000000     20.000000          20000.000000   \n",
      "25%     1816.000000      0.000000     35.000000          29498.750000   \n",
      "50%     1886.000000      0.000000     49.000000          39067.000000   \n",
      "75%     1955.000000      0.659520     64.000000          48544.250000   \n",
      "max     2999.000000  37123.850352     79.000000          57999.000000   \n",
      "\n",
      "               Coal           Oil           Gas        Cement       Flaring  \\\n",
      "count  21797.000000  21774.000000  21717.000000  20488.000000  21338.000000   \n",
      "mean     127.387271    153.480038    125.162671     62.599364     56.074327   \n",
      "std      677.951392    670.830891    514.391435    353.918064    337.629062   \n",
      "min        0.000000      0.000000      0.000000      0.000000      0.000000   \n",
      "25%        0.000000      0.113584      0.000000      0.000000      0.000000   \n",
      "50%        0.344416      1.344688      0.000000      0.032709      0.000000   \n",
      "75%        8.500480     13.008908      1.683510      0.725079      0.000000   \n",
      "max    15051.512770  12345.653374   7921.829472   2982.000000   2994.000000   \n",
      "\n",
      "             Other    Per Capita  \n",
      "count  2685.000000  19392.000000  \n",
      "mean    849.395127    121.565443  \n",
      "std    1055.250022    489.339877  \n",
      "min       0.000000      0.000000  \n",
      "25%       1.016000      0.231702  \n",
      "50%       9.237945      1.601436  \n",
      "75%    1812.000000      6.305100  \n",
      "max    2999.000000   2997.000000  \n",
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 63104 entries, 0 to 63103\n",
      "Data columns (total 13 columns):\n",
      " #   Column                Non-Null Count  Dtype  \n",
      "---  ------                --------------  -----  \n",
      " 0   Country               61087 non-null  object \n",
      " 1   ISO 3166-1 alpha-3    59483 non-null  object \n",
      " 2   Year                  62307 non-null  float64\n",
      " 3   Total                 62381 non-null  float64\n",
      " 4   Temperature           63104 non-null  int64  \n",
      " 5   GDP Per Capita (USD)  63104 non-null  int64  \n",
      " 6   Coal                  21797 non-null  float64\n",
      " 7   Oil                   21774 non-null  float64\n",
      " 8   Gas                   21717 non-null  float64\n",
      " 9   Cement                20488 non-null  float64\n",
      " 10  Flaring               21338 non-null  float64\n",
      " 11  Other                 2685 non-null   float64\n",
      " 12  Per Capita            19392 non-null  float64\n",
      "dtypes: float64(9), int64(2), object(2)\n",
      "memory usage: 6.3+ MB\n"
     ]
    }
   ],
   "source": [
    "penguins_df = pd.read_csv(\"noisy_datasets/penguins.csv\")\n",
    "diamonds_df = pd.read_csv(\"noisy_datasets/diamond.csv\")\n",
    "emissions_df = pd.read_csv(\"noisy_datasets/emissions_by_country.csv\")\n",
    "\n",
    "print(\"Penguin Data \\n\", penguins_df.describe())\n",
    "print(\"\\nDiamond Data \\n\", diamonds_df.describe())\n",
    "print(\"\\nEmissions Dataset\\n\", emissions_df.describe())\n",
    "emissions_df.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 344 entries, 0 to 343\n",
      "Data columns (total 10 columns):\n",
      " #   Column                  Non-Null Count  Dtype  \n",
      "---  ------                  --------------  -----  \n",
      " 0   species                 333 non-null    object \n",
      " 1   island                  334 non-null    object \n",
      " 2   calorie requirement     344 non-null    int64  \n",
      " 3   average sleep duration  344 non-null    int64  \n",
      " 4   bill_length_mm          337 non-null    float64\n",
      " 5   bill_depth_mm           333 non-null    float64\n",
      " 6   flipper_length_mm       336 non-null    float64\n",
      " 7   body_mass_g             339 non-null    float64\n",
      " 8   gender                  327 non-null    object \n",
      " 9   year                    342 non-null    float64\n",
      "dtypes: float64(5), int64(2), object(3)\n",
      "memory usage: 27.0+ KB\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/xj/24kvg1ds7mv3ntbbfv1kwrlh0000gn/T/ipykernel_33273/2452585692.py:11: FutureWarning: Automatic reindexing on DataFrame vs Series comparisons is deprecated and will raise ValueError in a future version. Do `left, right = left.align(right, axis=1, copy=False)` before e.g. `left == right`\n",
      "  outliers = (penguins_df < (Q1 - 1.5 * IQR)) | (penguins_df > (Q3 + 1.5 * IQR))\n"
     ]
    },
    {
     "ename": "AttributeError",
     "evalue": "'Series' object has no attribute 'wy_y_hatatere'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "Input \u001b[0;32mIn [3]\u001b[0m, in \u001b[0;36m<cell line: 12>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     11\u001b[0m outliers \u001b[38;5;241m=\u001b[39m (penguins_df \u001b[38;5;241m<\u001b[39m (Q1 \u001b[38;5;241m-\u001b[39m \u001b[38;5;241m1.5\u001b[39m \u001b[38;5;241m*\u001b[39m IQR)) \u001b[38;5;241m|\u001b[39m (penguins_df \u001b[38;5;241m>\u001b[39m (Q3 \u001b[38;5;241m+\u001b[39m \u001b[38;5;241m1.5\u001b[39m \u001b[38;5;241m*\u001b[39m IQR))\n\u001b[1;32m     12\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m column \u001b[38;5;129;01min\u001b[39;00m penguins_df\u001b[38;5;241m.\u001b[39mselect_dtypes(include\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mnumber\u001b[39m\u001b[38;5;124m'\u001b[39m)\u001b[38;5;241m.\u001b[39mcolumns:\n\u001b[0;32m---> 13\u001b[0m     penguins_df[column] \u001b[38;5;241m=\u001b[39m \u001b[43mpenguins_df\u001b[49m\u001b[43m[\u001b[49m\u001b[43mcolumn\u001b[49m\u001b[43m]\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mwy_y_hatatere\u001b[49m(\u001b[38;5;241m~\u001b[39moutliers[column], penguins_df[column]\u001b[38;5;241m.\u001b[39mmedian())\n",
      "File \u001b[0;32m~/opt/anaconda3/envs/workplease/lib/python3.8/site-packages/pandas/core/generic.py:5575\u001b[0m, in \u001b[0;36mNDFrame.__getattr__\u001b[0;34m(self, name)\u001b[0m\n\u001b[1;32m   5568\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m (\n\u001b[1;32m   5569\u001b[0m     name \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_internal_names_set\n\u001b[1;32m   5570\u001b[0m     \u001b[38;5;129;01mand\u001b[39;00m name \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_metadata\n\u001b[1;32m   5571\u001b[0m     \u001b[38;5;129;01mand\u001b[39;00m name \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_accessors\n\u001b[1;32m   5572\u001b[0m     \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_info_axis\u001b[38;5;241m.\u001b[39m_can_hold_identifiers_and_holds_name(name)\n\u001b[1;32m   5573\u001b[0m ):\n\u001b[1;32m   5574\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m[name]\n\u001b[0;32m-> 5575\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mobject\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[38;5;21;43m__getattribute__\u001b[39;49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mname\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[0;31mAttributeError\u001b[0m: 'Series' object has no attribute 'wy_y_hatatere'"
     ]
    }
   ],
   "source": [
    "penguins_df.info()\n",
    "penguins_df = penguins_df.dropna(subset=['species', 'island', 'gender'])\n",
    "penguins_df = penguins_df.fillna(penguins_df.select_dtypes(include='number').mean())\n",
    "penguins_df[['species', 'island', 'gender']] = penguins_df[['species', 'island', 'gender']].applymap(str.lower)\n",
    "penguins_df = penguins_df.dropna(subset=['species', 'island', 'gender'])\n",
    "penguins_df = penguins_df.fillna(penguins_df.select_dtypes(include='number').mean())\n",
    "penguins_df[['species', 'island', 'gender']] = penguins_df[['species', 'island', 'gender']].applymap(str.lower)\n",
    "Q1 = penguins_df.quantile(0.25)\n",
    "Q3 = penguins_df.quantile(0.75)\n",
    "IQR = Q3 - Q1\n",
    "outliers = (penguins_df < (Q1 - 1.5 * IQR)) | (penguins_df > (Q3 + 1.5 * IQR))\n",
    "for column in penguins_df.select_dtypes(include='number').columns:\n",
    "    penguins_df[column] = penguins_df[column].where(~outliers[column], penguins_df[column].median())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(10, 6))\n",
    "sns.barplot(x='gender', y='body_mass_g', data=penguins_df, estimator=np.mean, palette='muted')\n",
    "plt.title('Average Body Mass Distribution by Gender')\n",
    "plt.xlabel('Gender')\n",
    "plt.ylabel('Average Body Mass (g)')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(10, 6))\n",
    "sns.barplot(x='species', y='bill_length_mm', data=penguins_df, estimator=np.mean, palette='muted')\n",
    "plt.title('Average Bill Length by Species')\n",
    "plt.xlabel('Species')\n",
    "plt.ylabel('Average Bill Length (mm)')\n",
    "plt.xticks(rotation=45)\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(10, 6))\n",
    "sns.boxplot(x='species', y='calorie requirement', data=penguins_df, palette='muted')\n",
    "plt.title('Calorie Requirement Distribution by Species')\n",
    "plt.xlabel('Species')\n",
    "plt.ylabel('Calorie Requirement')\n",
    "plt.xticks(rotation=45)\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(10, 6))\n",
    "sns.countplot(x='island', hue='gender', data=penguins_df, palette='muted')\n",
    "plt.title('Gender Distribution by Island')\n",
    "plt.xlabel('Island')\n",
    "plt.ylabel('Count')\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "correlation_matrix = penguins_df.corr()\n",
    "plt.figure(figsize=(10, 8))\n",
    "sns.heatmap(correlation_matrix, annot=True)\n",
    "plt.title('Correlation Matrix')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "penguins_df = pd.get_dummies(penguins_df, columns=['species', 'island'], drop_first=False)\n",
    "penguins_df['gender_target'] = penguins_df['gender'].map({'female': 0, 'male': 1})\n",
    "correlation_matrix = penguins_df.corr()\n",
    "target_correlation = correlation_matrix['gender_target']\n",
    "print(target_correlation)\n",
    "threshold = 0.1 \n",
    "low_correlation_features = target_correlation[target_correlation.abs() < threshold].index.tolist()\n",
    "print(low_correlation_features)\n",
    "corelated_df = penguins_df.drop(columns=low_correlation_features)\n",
    "corelated_df.head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "non_categorical_columns = ['bill_length_mm', 'bill_depth_mm', 'flipper_length_mm', 'body_mass_g']\n",
    "min_values = corelated_df[non_categorical_columns].min()\n",
    "max_values = corelated_df[non_categorical_columns].max()\n",
    "corelated_df[non_categorical_columns] = (corelated_df[non_categorical_columns] - min_values) / (max_values - min_values)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "corelated_df.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Logit_Regression():\n",
    "    def __init__(self, learning_rate, iterations_count, weights):\n",
    "        self.learning_rate = learning_rate\n",
    "        self.iterations_count = iterations_count\n",
    "        self.weights = weights\n",
    "        self.loss = []\n",
    "        self.bias = None\n",
    "\n",
    "    def sigmoid(self, z):\n",
    "        return 1 / (1 + np.exp(-z))\n",
    "    \n",
    "    def cost(self, y, y_hat):\n",
    "        N = len(y)\n",
    "        return (1/N) * np.sum(-y * np.log(y_hat) - (1 - y) * np.log(1 - y_hat))\n",
    "    \n",
    "    def gradient_descent(self,X, y):\n",
    "        N = len(y)\n",
    "        self.bias = 1\n",
    "        weights_trans = np.transpose(self.weights)\n",
    "        z = np.dot(weights_trans, X) + self.bias\n",
    "        y_hat = self.sigmoid(z)\n",
    "        delta = y_hat - y\n",
    "        x_trans = np.transpose(X)\n",
    "        dW = (x_trans*delta)/N\n",
    "        w = w - self.learning_rate*dW\n",
    "        return w, y_hat\n",
    "\n",
    "    def fit(self,X,y):\n",
    "        best_weights = None\n",
    "        best_bias = None\n",
    "        best_acc = 0\n",
    "        for i in range(self.iterations_count):\n",
    "            self.weights, y_hat = self.gradient_descent(X,y)\n",
    "            c = self.cost(y, y_hat)\n",
    "            self.loss.append(c)\n",
    "            y_hat = self.predict(X)\n",
    "            acc = self.accuracy(y,y_hat)\n",
    "            if acc > best_acc:\n",
    "                best_acc = acc\n",
    "                best_weights = self.weights\n",
    "                best_bias = self.bias\n",
    "            \n",
    "            print(f\"Iteration {i}:\\nLoss is {c}\\nAccuracy is {acc}\\n\")\n",
    "        \n",
    "        updated_weights = {'weights': best_weights, 'bias': best_bias}\n",
    "        with open('best_weights_file.pkl', 'wb') as f:\n",
    "            pickle.dump(updated_weights, f)\n",
    "        print(\"Best Accuracy during training: \", best_acc * 100)\n",
    "\n",
    "    def predict(self, X):\n",
    "        self.bias = 1\n",
    "        weights_trans = np.transpose(self.weights)\n",
    "        z = np.dot(weights_trans, X) + self.bias\n",
    "        y_hat = self.sigmoid(z)\n",
    "        return y_hat\n",
    "    \n",
    "    def accuracy(self, y, y_hat):\n",
    "        y_hat_bin = (y_hat > 0.5).astype(int)\n",
    "        y_bin = (y > 0.5).astype(int)\n",
    "        predictions = np.sum(y_hat_bin == y_bin)\n",
    "        acc = predictions / len(y)\n",
    "        return acc\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = corelated_df[['bill_length_mm', 'bill_depth_mm', 'flipper_length_mm', 'body_mass_g']].values\n",
    "y = corelated_df['gender_target'].values.reshape(-1, 1) \n",
    "N = X.shape[0] \n",
    "train_size = int(0.8 * N) \n",
    "index_number = np.arange(N)\n",
    "np.random.shuffle(index_number)\n",
    "train_indices = index_number[:train_size]\n",
    "test_indices = index_number[train_size:]\n",
    "X_train = X[train_indices]\n",
    "y_train = y[train_indices]\n",
    "X_test = X[test_indices]\n",
    "y_test = y[test_indices]\n",
    "print(\"X_train shape:\", X_train.shape)\n",
    "print(\"y_train shape:\", y_train.shape)\n",
    "print(\"X_test shape:\", X_test.shape)\n",
    "print(\"y_test shape:\", y_test.shape)\n",
    "weights = np.random.uniform(0, 1)\n",
    "learning_rate = 0.001\n",
    "iterations_count = 1000000\n",
    "model = Logit_Regression(learning_rate, iterations_count, weights)\n",
    "model.fit(X_train, y_train)\n",
    "\n",
    "y_pred = model.predict(X_test)\n",
    "test_accuracy = model.evaluate_accuracy(y_test, y_pred)\n",
    "print(f\"Test Accuracy: \",test_accuracy * 100)\n",
    "model.plot_loss()\n",
    "with open('best_model_weights.pkl', 'rb') as f:\n",
    "    saved_model = pickle.load(f)\n",
    "    print(f\"Saved Weights: \",saved_model['weights'])\n",
    "    print(f\"Saved Bias: \",saved_model['bias'])\n",
    "\n",
    "print(\"hahaha\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "workplease",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
