{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np \n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "import pickle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "corelated_df = pd.read_csv(\"penguins_corelated_task_2.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Logit_Regression():\n",
    "    def __init__(self, learning_rate, iterations_count):\n",
    "        self.learning_rate = learning_rate\n",
    "        self.iterations_count = iterations_count\n",
    "        self.weights = None\n",
    "        self.loss = []\n",
    "        self.bias = None\n",
    "\n",
    "    def sigmoid(self, z):\n",
    "        return 1 / (1 + np.exp(-z))\n",
    "    \n",
    "    def cost(self, y, y_hat):\n",
    "        N = len(y)\n",
    "        return (1/N) * np.sum(-y * np.log(y_hat) - (1 - y) * np.log(1 - y_hat))\n",
    "    \n",
    "    def gradient_descent(self,X, y):\n",
    "        N = len(y)\n",
    "        z = np.dot(X, self.weights) + self.bias\n",
    "        y_hat = self.sigmoid(z)\n",
    "        delta = y_hat - y\n",
    "        x_trans = np.transpose(X)\n",
    "        dW = np.dot(x_trans,delta)/N\n",
    "        db = np.sum(delta)/N\n",
    "        self.bias = self.bias - self.learning_rate*db\n",
    "        self.weights = self.weights - self.learning_rate*dW\n",
    "        return self.weights, y_hat\n",
    "\n",
    "    def fit(self,X,y):\n",
    "        self.weights = np.random.uniform(0, 1, size=(X.shape[1], 1))\n",
    "        self.bias = 0\n",
    "        for i in range(self.iterations_count):\n",
    "            self.weights, y_hat = self.gradient_descent(X,y)\n",
    "            c = self.cost(y, y_hat)\n",
    "            self.loss.append(c)\n",
    "            print(f\"Iteration {i}:\\nLoss is {c}\\n\")\n",
    "\n",
    "    def predict(self, X):\n",
    "        z = np.dot(X, self.weights) + self.bias\n",
    "        y_hat = self.sigmoid(z)\n",
    "        return y_hat\n",
    "    \n",
    "    def accuracy(self, y, y_hat):\n",
    "        y_hat_bin = (y_hat > 0.5).astype(int)\n",
    "        y_bin = (y > 0.5).astype(int)\n",
    "        predictions = np.sum(y_hat_bin == y_bin)\n",
    "        acc = predictions / len(y)\n",
    "        return acc\n",
    "    \n",
    "    def loss_graph(self):\n",
    "        plt.plot(range(self.iterations_count), self.loss)\n",
    "        plt.xlabel('Iterations')\n",
    "        plt.ylabel('Loss')\n",
    "        plt.title('Loss Over Iterations')\n",
    "        plt.show()\n",
    "    \n",
    "    def save_model_to_pickle(model, filename=\"model_weights.pkl\"):\n",
    "        with open(filename, 'wb') as f:\n",
    "            pickle.dump({'weights': model.weights, 'bias': model.bias}, f)\n",
    "        print(f\"Model weights saved to {filename}\")\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = corelated_df[['bill_length_mm', 'bill_depth_mm', 'flipper_length_mm', 'body_mass_g']].values\n",
    "y = corelated_df['gender_target'].values.reshape(-1, 1) \n",
    "N = X.shape[0] \n",
    "train_size = int(0.8 * N) \n",
    "index_number = np.arange(N)\n",
    "np.random.shuffle(index_number)\n",
    "train_indices = index_number[:train_size]\n",
    "test_indices = index_number[train_size:]\n",
    "X_train = X[train_indices]\n",
    "y_train = y[train_indices]\n",
    "X_test = X[test_indices]\n",
    "y_test = y[test_indices]\n",
    "print(\"X_train shape:\", X_train.shape)\n",
    "print(\"y_train shape:\", y_train.shape)\n",
    "print(\"X_test shape:\", X_test.shape)\n",
    "print(\"y_test shape:\", y_test.shape)\n",
    "\n",
    "\n",
    "learning_rate_0 = 0.001\n",
    "iterations_count_0 = 10000\n",
    "model_0 = Logit_Regression(learning_rate_0, iterations_count_0)\n",
    "model_0.fit(X_train, y_train)\n",
    "y_pred_0 = model_0.predict(X_test)\n",
    "test_accuracy_0 = model_0.accuracy(y_test, y_pred_0)\n",
    "\n",
    "learning_rate = 0.001\n",
    "iterations_count = 50000\n",
    "model = Logit_Regression(learning_rate, iterations_count)\n",
    "model.fit(X_train, y_train)\n",
    "y_pred = model.predict(X_test)\n",
    "test_accuracy = model.accuracy(y_test, y_pred)\n",
    "\n",
    "learning_rate_1 = 0.01\n",
    "iterations_count_1 = 100000\n",
    "model_1 = Logit_Regression(learning_rate_1, iterations_count_1)\n",
    "model_1.fit(X_train, y_train)\n",
    "y_pred_1 = model_1.predict(X_test)\n",
    "test_accuracy_1 = model_1.accuracy(y_test, y_pred_1)\n",
    "\n",
    "\n",
    "print(f\"Test Accuracy 0: \",test_accuracy_0 * 100)\n",
    "model_0.loss_graph()\n",
    "print(f\"Test Accuracy: \",test_accuracy * 100)\n",
    "model.loss_graph()\n",
    "print(f\"Test Accuracy 1: \",test_accuracy_1 * 100)\n",
    "model_1.loss_graph()\n",
    "\n",
    "best_acc = max(test_accuracy, test_accuracy_1, test_accuracy_0)\n",
    "print(f\"Best Accuracy: \",best_acc * 100)\n",
    "if best_acc == test_accuracy:\n",
    "    Logit_Regression.save_model_to_pickle(model, filename=\"model_weights.pkl\")\n",
    "elif best_acc == test_accuracy_1:\n",
    "    Logit_Regression.save_model_to_pickle(model_1, filename=\"model_weights.pkl\")\n",
    "else: \n",
    "    Logit_Regression.save_model_to_pickle(model_0, filename=\"model_weights.pkl\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "workplease",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
